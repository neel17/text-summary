{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text summary.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/neel17/text-summary/blob/master/text_summary.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "aDsbJF8wcMq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/xiaoxu193/PyTeaser"
      ]
    },
    {
      "metadata": {
        "id": "SMs-ta5fayEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b264FXHDaJdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "1f6da0fc-95cf-45fd-f146-d5064ca44e0e"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyteaser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyteaser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/7a/310592c6e7998440e56a8650446ecf3ded076431415c60f0f3b946b54462/pyteaser-2.0.tar.gz (40kB)\n",
            "\u001b[K    100% |████████████████████████████████| 40kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python2.7/dist-packages (from pyteaser) (4.0.0)\n",
            "Collecting lxml (from pyteaser)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/14/f4343239f955442da9da1919a99f7311bc5627522741bada61b2349c8def/lxml-4.2.5-cp27-cp27mu-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 3.6MB/s \n",
            "\u001b[?25hCollecting cssselect (from pyteaser)\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Collecting jieba (from pyteaser)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.3MB 4.1MB/s \n",
            "\u001b[?25hCollecting beautifulsoup (from pyteaser)\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/ee/295988deca1a5a7accd783d0dfe14524867e31abb05b6c0eeceee49c759d/BeautifulSoup-3.2.1.tar.gz\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow->pyteaser) (0.46)\n",
            "Building wheels for collected packages: pyteaser, jieba, beautifulsoup\n",
            "  Running setup.py bdist_wheel for pyteaser ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/89/83/30/3325baa3ddc421ed9d35472d271adc825fad58bc2a453bc731\n",
            "  Running setup.py bdist_wheel for jieba ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c9/c7/63/a9ec0322ccc7c365fd51e475942a82395807186e94f0522243\n",
            "  Running setup.py bdist_wheel for beautifulsoup ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/74/d2/0b/8ef02aab9e15c6e5158d7aee909adab931a9c54920e99f468e\n",
            "Successfully built pyteaser jieba beautifulsoup\n",
            "Installing collected packages: lxml, cssselect, jieba, beautifulsoup, pyteaser\n",
            "Successfully installed beautifulsoup-3.2.1 cssselect-1.0.3 jieba-0.39 lxml-4.2.5 pyteaser-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r22A3grgazNR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8ApldUdacB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "96719b38-0c7a-4682-8c0f-a6851ea61600"
      },
      "cell_type": "code",
      "source": [
        "from pyteaser import SummarizeUrl\n",
        "#url = 'http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html'\n",
        "summaries = SummarizeUrl(url)\n",
        "print summaries"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'RCNN, Fast RCNN and Faster RCNN. In the upcoming article of this series, we will cover more advanced algorithms like YOLO, SSD, etc.', u'In this article specifically, we will dive deeper and look at various algorithms that can be used for object detection.', u'In the next article of this series, we will encounter modern object detection algorithms such as YOLO and RetinaNet.', u'While this was a simple example, the applications of object detection span multiple and diverse industries, from round-the-clock surveillance to real-time vehicle detection in smart cities.', u'Object detection is a fascinating field, and is rightly seeing a ton of traction in commercial, as well as research applications.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y-2AVYoKacBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "27c0c2b1-b245-495d-e1af-d10c75d2fd27"
      },
      "cell_type": "code",
      "source": [
        "summaries"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'RCNN, Fast RCNN and Faster RCNN. In the upcoming article of this series, we will cover more advanced algorithms like YOLO, SSD, etc.',\n",
              " u'In this article specifically, we will dive deeper and look at various algorithms that can be used for object detection.',\n",
              " u'In the next article of this series, we will encounter modern object detection algorithms such as YOLO and RetinaNet.',\n",
              " u'While this was a simple example, the applications of object detection span multiple and diverse industries, from round-the-clock surveillance to real-time vehicle detection in smart cities.',\n",
              " u'Object detection is a fascinating field, and is rightly seeing a ton of traction in commercial, as well as research applications.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "8jRLfom5ab72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "bec014c0-cd1e-4855-a84b-5337d655da7f"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://towardsdatascience.com/training-machine-learning-models-online-for-free-gpu-tpu-enabled-5def6a5c1ce3')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'Computation power needed to train machine learning and deep learning model on large datasets, has always been a huge hindrance for machine learning enthusiast.',\n",
              " u'Colaboratory is a google research project created to help disseminate machine learning education and research.',\n",
              " u'In this post I will providing information about the various service that gives us the computation power to us for training models.',\n",
              " u'The notebooks are connected to your google drive, so you can acess it any time you want,and also upload or download notebook from github.',\n",
              " u'But with jupyter notebook which run on cloud anyone who is has the passion to learn can train and come up with great results.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Ki9dZ78Rab65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a5850b25-4c28-4079-c391-223352d5c789"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://towardsdatascience.com/advanced-reinforcement-learning-6d769f529eb3')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'One of my favorite things about deep reinforcement learning is that, unlike supervised learning, it really, really doesn\\u2019t want to work.',\n",
              " u'Deep RL models are really hard to train, period.',\n",
              " u'For this reason, a lot of early successes in deep RL (e.g.',\n",
              " u'The low-level policy is trained to take environment actions that would produce a state observation similar to the given goal state.',\n",
              " u'Traditionally in RL, we can either do model-free learning or model-based learning.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "pqJUyQ-Iab11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8c7321f6-5d9f-4551-a632-b22134a9827c"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'In August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post \\u201cText summarization with TensorFlow\\u201d.',\n",
              " u'Peter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research.',\n",
              " u'Text summarization problem has many useful applications.',\n",
              " u'Readme on GitHub repo lists a sequence of commands to run training and testing code.',\n",
              " u'The dataset contains about 10 million documents.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "vXcdotFKab1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBuTZMNSDTXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "543c2949-d4e7-4202-ebb9-1cf56d2c27c6"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'In the past, I have used GloVe for this, but I found another set of word embeddings, called ConceptNet Numberbatch (CN).',\n",
              " u'To help train the model faster, we are going to sort the reviews by the length of the descriptions from shortest to longest.',\n",
              " u'Some reviews will not be included because of the number of UNK tokens that are in the description or summary.',\n",
              " u' Fewer than 0.7% of words are UNKs, so not many reviews will be removed.',\n",
              " u'Based on the work of its creators, it seems to outperform GloVe, which makes sense because CN is an ensemble of embeddings that includes GloVe.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "sAhXBJTD4Dh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9e35d794-f381-4ec5-d417-b4216d223274"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://www.reuters.com/article/us-saudi-khashoggi-merkel/no-arms-for-riyadh-while-khashoggi-questions-remain-germanys-merkel-idUSKCN1MV0ST?feedName=worldNews&utm_campaign=fullarticle&feedType=RSS&utm_medium=referral&utm_source=inshorts')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'BERLIN (Reuters) - Germany will not export arms to Saudi Arabia while the current uncertainty over the fate of journalist Jamal Khashoggi persists, Chancellor Angela Merkel said on Sunday.',\n",
              " u'Campaigning for her party in a regional election, Merkel repeated to a news conference her earlier condemnation of Khashoggi\\u2019s killing, which Saudi Arabia admitted had taken place inside its consulate in Istanbul.\\n\\n\\u201cFirst, we condemn this act in the strongest terms,\\u201d she said. \\u201cSecond, there is an urgent need to clarify what happened - we are far from this having been cleared up and those responsible held to account ...',\n",
              " u' As far as arms exports are concerned, those can\\u2019t take place in the current circumstances.\\u201d']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "CuNQdNrgDcib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "3d39c5ee-b012-4f7f-a6bb-a5cf283f1eac"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b',5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6fd8e7d04a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum_text_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sum_text_summary' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHIjmdWKErRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "11432f9e-0ab3-4309-925d-ec52b5eb06c2"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://machinelearningmastery.com/gentle-introduction-text-summarization/')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'In their 2014 book on the subject titled \\u201cAutomatic Text Summarization,\\u201d the authors provide 6 reasons why we need automatic text summarization tools.',\n",
              " u'Automatic text summarization, or just text summarization, is the process of creating a short and coherent version of a longer document.',\n",
              " u'In their 1999 book on the topic titled \\u201cAdvances in Automatic Text Summarization,\\u201d the authors provide a useful list of every-day examples of text summarization.',\n",
              " u'Now that we know that we need automatic summaries of text, let\\u2019s better define what we mean by text summarization.',\n",
              " u'Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "PIGBwSN9E15E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ce07d519-c128-4958-e635-c094e0757048"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://machinelearningmastery.com/gentle-introduction-text-summarization/',5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is a great need to reduce much of this text data to shorter, focused summaries that capture the salient details, both so we can navigate it more effectively as well as check whether the larger documents contain the information that we are looking for.\n",
            "Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning\n",
            "digests (TV guide) biography (resumes, obituaries) abridgments (Shakespeare for children) bulletins (weather forecasts/stock market reports) sound bites (politicians on a current issue) histories (chronologies of salient events)\n",
            "Abstractive text summarization is the task of generating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage.\n",
            "… the recent success of sequence-to-sequence models, in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULkQKvA-3hji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "171c6ef1-81bb-4be0-dd91-1d81938734f2"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://www.reuters.com/article/us-saudi-khashoggi-merkel/no-arms-for-riyadh-while-khashoggi-questions-remain-germanys-merkel-idUSKCN1MV0ST?feedName=worldNews&utm_campaign=fullarticle&feedType=RSS&utm_medium=referral&utm_source=inshorts',5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-36be1187f077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum_text_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.reuters.com/article/us-saudi-khashoggi-merkel/no-arms-for-riyadh-while-khashoggi-questions-remain-germanys-merkel-idUSKCN1MV0ST?feedName=worldNews&utm_campaign=fullarticle&feedType=RSS&utm_medium=referral&utm_source=inshorts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sum_text_summary' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9hTORKKD824s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Using summy\n",
        "[Summy Github](https://github.com/miso-belica/sumy)"
      ]
    },
    {
      "metadata": {
        "id": "hk7inkLKabvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6262affc-5d76-44e4-aede-50aa863225da"
      },
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/0e/30ebd2fb0925537a3b2f9fccf0a13171ba557e9450b1702d278159d3c592/sumy-0.7.0-py2.py3-none-any.whl (78kB)\n",
            "\r\u001b[K    13% |████▏                           | 10kB 11.3MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 20kB 1.9MB/s eta 0:00:01\r\u001b[K    39% |████████████▌                   | 30kB 2.3MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 40kB 2.1MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 51kB 2.3MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 61kB 2.7MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 71kB 3.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python2.7/dist-packages (from sumy) (2.18.4)\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from sumy) (3.2.5)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy) (2018.8.24)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python2.7/dist-packages (from breadability>=0.1.20->sumy) (4.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk>=3.0.2->sumy) (1.11.0)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Running setup.py bdist_wheel for breadability ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "  Running setup.py bdist_wheel for docopt ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 sumy-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6uMWdbDK9tHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "999896a4-dd0a-4350-efe0-dbadba5285db"
      },
      "cell_type": "code",
      "source": [
        "! pip install git+git://github.com/miso-belica/sumy.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/miso-belica/sumy.git\n",
            "  Cloning git://github.com/miso-belica/sumy.git to /tmp/pip-req-build-450dpy\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy==0.7.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
            "Collecting breadability>=0.1.20 (from sumy==0.7.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python2.7/dist-packages (from sumy==0.7.0) (2.18.4)\n",
            "Collecting pycountry>=18.2.23 (from sumy==0.7.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/99/a81f1da10070c563450a4ef334622e72fb0b7b74c929fdbed711154d8416/pycountry-18.5.26-py2-none-any.whl (10.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.3MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from sumy==0.7.0) (3.2.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python2.7/dist-packages (from breadability>=0.1.20->sumy==0.7.0) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python2.7/dist-packages (from breadability>=0.1.20->sumy==0.7.0) (4.2.5)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy==0.7.0) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy==0.7.0) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->sumy==0.7.0) (2018.8.24)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk>=3.0.2->sumy==0.7.0) (1.11.0)\n",
            "Building wheels for collected packages: sumy, docopt, breadability\n",
            "  Running setup.py bdist_wheel for sumy ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-gQHpp4/wheels/ca/d7/7c/6e918e8342b863a32773e60f127b2714872821e5604936c0ea\n",
            "  Running setup.py bdist_wheel for docopt ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
            "  Running setup.py bdist_wheel for breadability ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built sumy docopt breadability\n",
            "Installing collected packages: docopt, breadability, pycountry, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-18.5.26 sumy-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yFciEQ-F9R65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###using Python API"
      ]
    },
    {
      "metadata": {
        "id": "cTtP7hh_9r4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1949fb73-7ada-4b48-c29d-2882f35820a7"
      },
      "cell_type": "code",
      "source": [
        "!python -c \"import nltk; nltk.download('punkt')\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M3TleobRabu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "74985f75-c80e-406b-e094-3263dc08a450"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "LANGUAGE = 'english'\n",
        "# \"czech\"\n",
        "SENTENCES_COUNT = 5\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/\"\n",
        "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "    # or for plain text files\n",
        "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "    summarizer = Summarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "        print(sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With push notifications and article digests gaining more and more traction, the task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem.\n",
            "One way to interpret this spatial decomposition operation is that singular vectors can capture and represent word combination patterns which are recurring in the corpus.\n",
            "Unlike ROUGE, BLEU directly accounts for variable length phrases – unigrams, bigrams, trigrams etc., by taking a weighted average.\n",
            "To compare different tweaks to the neural network architecture we had to resort to using a mathematical measure of the model fit on the training set “running average loss” .\n",
            "Due to lack of GPU resources and a lot of parameters to tune we end our research on abstractive summarization at a point where we cannot conclude with absolute certainty that the model can be used as an alternative to current extractive implementations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bW55E-hLabpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "460a278e-b3cd-4fc2-8a6d-a574f13a3f39"
      },
      "cell_type": "code",
      "source": [
        "!sumy lex-rank --length=3 --url=http://en.wikipedia.org/wiki/Automatic_summarization # what's summarization?"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Learning Algorithms for Keyphrase Extraction\".\n",
            "Automatic Keyphrases Extraction .\n",
            "\"Summarizing Conceptual Graphs for Automatic Summarization Task\".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRkw13Bw_Cq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "4cb65ec6-e315-48c9-ef00-1e6ea02916e4"
      },
      "cell_type": "code",
      "source": [
        "!sumy lex-rank --length=5 --url=https://towardsdatascience.com/advanced-reinforcement-learning-6d769f529eb3"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "μ_lo, the low-level policy, outputs environment actions in an attempt to reach that goal state observation.Here’s the idea: we have 2 layers of policy.\n",
            "The low-level policy is trained to take environment actions that would produce a state observation similar to the given goal state .\n",
            "However, old experience cannot be used directly to train the high-level policy.\n",
            "It is also responsible for passing relevant memories to the policy, which uses those memories and the current state to output actions.\n",
            "In model-based RL, we first learn a transition model of the environment based on raw observations, and then use that model to choose actions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UDenigJa_V8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# from __future__ import absolute_import\n",
        "# from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# from sumy.parsers.html import HtmlParser\n",
        "# from sumy.parsers.plaintext import PlaintextParser\n",
        "# from sumy.nlp.tokenizers import Tokenizer\n",
        "# from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "# from sumy.nlp.stemmers import Stemmer\n",
        "# from sumy.utils import get_stop_words\n",
        "\n",
        "def sum_text_summary(url,sentence_count):\n",
        "  LANGUAGE = 'english'\n",
        "  # \"czech\"\n",
        "  SENTENCES_COUNT = sentence_count\n",
        "\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      url = url\n",
        "      parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "      # or for plain text files\n",
        "      # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "      stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "      summarizer = Summarizer(stemmer)\n",
        "      summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "      for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "          print(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILNdTFND_xDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "34b97295-29bf-4b45-8c81-b55ca5fe2f00"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://towardsdatascience.com/advanced-reinforcement-learning-6d769f529eb3', 4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Both DQN and A3C/A2C can be powerful baseline agents, but they tend to suffer when faced with more complex tasks, severe partial observability, and/or long delays between actions and relevant reward signals.\n",
            "This is intuitive in everyday life: when I decide to cook a meal (which is basically never, by the way, but for the sake of argument let us assume I am a responsible human being), I am able to divide this task into simpler sub-tasks: chopping vegetables, boiling pasta, etc.\n",
            "On the technical RL front, HRL is especially appealing because it helps address two of the biggest challenges I mentioned under our second question, i.e. how to learn from experience effectively: long-term credit assignment and sparse reward signals .\n",
            "Another Medium post has done a great job of exploring this idea, so I won’t repeat it all here, but the key argument is that our brain likely does not function as an “input-output” machine, like most neural nets are interpreted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c68s_TR9AXor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7a06e813-ccb1-4571-d302-850e2f8fa8bf"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/',4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With push notifications and article digests gaining more and more traction, the task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem.\n",
            "Unlike ROUGE, BLEU directly accounts for variable length phrases – unigrams, bigrams, trigrams etc., by taking a weighted average.\n",
            "To compare different tweaks to the neural network architecture we had to resort to using a mathematical measure of the model fit on the training set “running average loss” .\n",
            "Due to lack of GPU resources and a lot of parameters to tune we end our research on abstractive summarization at a point where we cannot conclude with absolute certainty that the model can be used as an alternative to current extractive implementations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uR-TheEkGj1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "1033f412-0ed4-4060-97ce-8db0f5fbc573"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://rasa.com/docs/nlu/dataformat/#section-dataformat',5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training data can either be stored in a single file or split into multiple files.\n",
            "This can make it easier to keep things organised, or to share data between projects.\n",
            "For example, if you have a restaurant bot which can also handle some basic smalltalk,\n",
            "you could have a folder called nlu_data :\n",
            "To train a model with this data, pass the path to the directory to the train script:\n",
            "Splitting the training data into multiple files currently only works for markdown and JSON data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUkMF0FGIYCh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Financial Annual Report Summarization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VW614XRbHPzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "68d24552-5649-4774-d26f-956728f78de6"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://www.infosys.com/investors/reports-filings/annual-report/annual/Documents/AR-2018/boards-report.html',20)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Consequent to the above, in the consolidated financials, the Company has written down the entire carrying value of the investment in its associate, DWA Nova LLC, amounting to ₹ 71 crore.\n",
            "On account of the conclusion of APA with the IRS, the Company has reversed income tax expense amounting to US$ 225 million (₹ 1,432 crore) pertaining to previous periods resulting in an increase in profits for fiscal 2018.\n",
            "In accordance with the APA, the Company has reversed income tax expense provision of US$ 225 million (₹ 1,432 crore) which pertains to previous periods that are no longer required.\n",
            "Loans, guarantees and investments covered under Section 186 of the Companies Act, 2013 form part of the Notes to the financial statements provided in this Annual Report.\n",
            "Particulars of contracts or arrangements with related parties referred to in Section 188(1) of the Companies Act, 2013, in the prescribed Form AOC-2, is appended as Annexure 2 to the Board’s report.\n",
            "Our strategic objective is to build a sustainable organization that remains relevant to the agenda of our clients, while creating growth opportunities for our employees and generating profitable returns for our investors.\n",
            "We will continue to invest in research and development (R&D) to stay abreast of new technologies and to incubate new offerings in areas such as blockchain, AR / VR and speech, vision, video and image intelligence.\n",
            "We will continue to deploy our capital in making selective business acquisitions that augment our Agile Digital expertise, to complement our presence in certain market segments.\n",
            "We will continue to leverage these, along with lean processes, Agile development and our Global Delivery Model (GDM) to deliver solutions and services to our clients in the most cost-effective manner, while at the same time optimizing our cost structure to remain competitive.\n",
            "Further, we are expanding our relationships with universities around the world to curate specific curricula for our employees in areas such as creative design skills, machine learning, autonomous technologies, blockchain etc.\n",
            "The Fund partners with startups by providing early-stage capital and in helping bring their innovations to market, attaining scale, product validation and customer introductions.\n",
            "The new name is a reflection of the paradigm shift in the nature of services that the Company now offers through its holistic approach of end-to-end transformative BPM (Business Process Management).\n",
            "Infosys Limited as an enterprise is assessed for ISAE 3402 / SSAE 18 SOC 1 type II and has received an independent auditors’ assurance compliance report.\n",
            "We had over 200 leaders nominated across functions, subsidiaries and technologies, resulting in a diverse mix across locations, focused on leadership facets like executive presence, storytelling etc.\n",
            "We believe that a truly diverse board will leverage differences in thought, perspective, knowledge, skill, regional and industry experience, cultural and geographical background, age, ethnicity, race and gender, that will help us retain our competitive advantage.\n",
            "Salil Parekh as the Chief Executive Officer and Managing Director (CEO & MD) effective January 2, 2018, approved by shareholders vide a postal ballot concluded on February 20, 2018.\n",
            "The primary reason for seeking the proposed delisting is the low average daily trading volume of Infosys ADSs on these exchanges, which is not commensurate with the related administrative requirements.\n",
            "This year, the Foundation’s activities have extended from Jammu & Kashmir to Tamil Nadu, and from Gujarat to Arunachal Pradesh, with an emphasis on expanding our reach while ensuring focus on key areas of development.\n",
            "In fiscal 2018, Infosys Foundation USA advanced its mission to increase access to Computer Science (CS) and Maker education, with an emphasis on under-represented students.\n",
            "The chief guest, Prof. Kip Thorne, Nobel laureate in Physics for 2017, along with the jury chairs and trustees, gave away the prizes to the winners at a ceremony in Bengaluru on January 10, 2018.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8m_Bp3gLIDCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e705d516-47e1-478b-cb05-361ab9c68f8e"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://www.infosys.com/investors/reports-filings/annual-report/annual/Documents/AR-2018/boards-report.html')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'The Board of Directors hereby submits the report of the business and operations of your Company (\\u2018the Company\\u2019 or \\u2018Infosys\\u2019), along with the audited financial statements, for the financial year ended March 31, 2018.',\n",
              " u'The consolidated performance of the Company and its subsidiaries has been referred to wherever required.\\n\\n1.',\n",
              " u'Consequent to the above, in the consolidated financials, the Company has written down the entire carrying value of the investment in its associate, DWA Nova LLC, amounting to 71 crore.',\n",
              " u' The write-down in the carrying value of investment in associate DWA Nova LLC during the year ended March 31, 2017 was 18 crore.\\n\\n(5) Equity shares are at par value of 5 per share.',\n",
              " u'The disposal group does not constitute a separate major component of the Company and therefore, has not been classified as discontinued operations.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "dXnvnHfYZ48Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "4f07c951-05cc-4b8d-e95c-c6b584a49126"
      },
      "cell_type": "code",
      "source": [
        "sum_text_summary('https://cbinsights.us1.list-manage.com/track/click?u=0c60818e26ecdbe423a10ad2f&id=1129ed3f84&e=d59f404159',10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Companies are trying to remove the burden of downloading games by streaming them directly from remote servers.\n",
            "Recently, game publishers like Epic and Bethesda have helped drive the tremendous growth of the gaming industry.\n",
            "Last year, gaming generated nearly $122B in total revenue, and it’s expected to grow nearly 50% to reach $180B by 2021.\n",
            "These platforms allow gamers to stream their favorite games from remote cloud servers.\n",
            "This eliminates the need for expensive hardware (and often the need to download games at all).\n",
            "It can also turn most devices into powerful gaming machines.\n",
            "Private companies like Shadow , LiquidSky , and Vortex are strategically positioning themselves for a cloud-based gaming future, while corporates are also jumping on the trend by building out the necessary infrastructure.\n",
            "Using CB Insights’ patent search engine , we looked at the research efforts of 5 publicly traded companies preparing for this fundamental shift in gaming.\n",
            "Below, we dig into these patents, which hint at what the future of cloud-based gaming might look like.\n",
            "GAMING INDUSTRY Gaming companies are defined as those developing technologies for the PC, console, mobile, and/or AR/VR video gaming market.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ENtoPXQraSI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3e4ce6e6-0528-45d3-9002-40f57ef4d432"
      },
      "cell_type": "code",
      "source": [
        "SummarizeUrl('https://cbinsights.us1.list-manage.com/track/click?u=0c60818e26ecdbe423a10ad2f&id=1129ed3f84&e=d59f404159')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'Recently, game publishers like Epic and Bethesda\\xa0have helped drive the tremendous growth of the gaming industry.',\n",
              " u'Cloud wars: the race between Google, Microsoft, and Amazon See why the industry is expected to reach $513B by 2022.',\n",
              " u' Below, we\\xa0dig into these patents, which hint at what the future of\\xa0cloud-based gaming might look like.',\n",
              " u'Private companies like Shadow,\\xa0LiquidSky, and Vortex are strategically positioning themselves for a cloud-based gaming future, while corporates are also\\xa0jumping on the trend by\\xa0building out\\xa0the necessary infrastructure.',\n",
              " u'Last year, gaming generated\\xa0nearly\\xa0$122B in total revenue, and it\\u2019s expected to grow nearly 50% to reach $180B by 2021.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}